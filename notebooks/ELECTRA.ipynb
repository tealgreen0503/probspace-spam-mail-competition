{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ELECTRA.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1Okl9_BGyx4MX6AM_3FHF3w7Xp8nHkzNN","authorship_tag":"ABX9TyNJQzzOV4ltPnBPhqjwRmJK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KrAF3997hodk"},"source":["!pip install transformers nlp pulp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"elu65_j8CjQN"},"source":["%cd \"/content/drive/My Drive/Colab Notebooks/Competition/ProbSpace/Spam mail\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NYKsV_GvhNih"},"source":["import collections\n","import os\n","import random\n","import re\n","import time\n","\n","from bs4 import BeautifulSoup\n","import numpy as np\n","import pandas as pd\n","import plotly.express as px\n","import pulp\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import classification_report, f1_score, confusion_matrix\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from tqdm.notebook import tqdm\n","\n","from transformers import (\n","    AutoConfig,\n","    AutoTokenizer,\n","    AutoModel,\n","    AdamW,\n","    get_cosine_schedule_with_warmup\n",")\n","import nlp\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMaywe9tZxr8"},"source":["SEED = 44\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        \n","seed_everything(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvUQ3RD8gfue"},"source":["if torch.cuda.is_available():\n","    current_device = torch.cuda.current_device()\n","    print(\"Device:\", torch.cuda.get_device_name(current_device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zWSIxmYXiCPy"},"source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","TRAIN_FILE = \"./data/train_data.csv\"\n","TEST_FILE = \"./data/test_data.csv\"\n","MODEL_DIR = \"./checkpoint/\"\n","MODEL_NAME = 'google/electra-base-discriminator'\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 128\n","NUM_CLASS = 2\n","NUM_EPOCH = 10\n","NUM_SPLIT = 5\n","TEST_FREQS = [7838, 17000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gw5j8FB84fml"},"source":["def preprocess(text):\n","    text = BeautifulSoup(text, \"html.parser\").get_text(strip=strip)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0TbY7nylJjJu"},"source":["def make_dataset(df, tokenizer, device):\n","    dataset = nlp.Dataset.from_pandas(df)\n","    dataset = dataset.map(lambda example: tokenizer(example[\"contents\"]))\n","    dataset.rename_column_('y', 'labels')\n","    dataset.set_format(type='torch', \n","                       columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], \n","                       device=device)\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRJG35BDQaRN"},"source":["class Tokenizer:\n","    def __init__(self, model_name, additional_tokens=None, max_length=512):\n","        self.bert_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","        self.bert_tokenizer.add_tokens(additional_tokens)\n","        self.max_length = max_length\n","        assert self.max_length % 2 == 0\n","\n","    def __call__(self, text):\n","        sep_index = text.find(\"\\r\\n\")\n","        input = self.bert_tokenizer(text[:sep_index], text[sep_index:],\n","                                    padding='max_length', max_length=self.max_length)\n","        if len(input[\"input_ids\"]) > self.max_length:\n","            for k, v in input.items():\n","                input[k] = v[:self.max_length//4] + v[-(self.max_length//4)*3:]\n","        return input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kV2Ni6hUVi8"},"source":["class Classifier(nn.Module):\n","    def __init__(self, model_name, num_classes=2):\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(model_name, output_hidden_states=True, return_dict=False)\n","        self.bert = AutoModel.from_pretrained(model_name, config=config)\n","        self.dropout = nn.Dropout(0.2)\n","        self.high_dropout = nn.Dropout(p=0.5)\n","        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n","\n","        n_weights = config.num_hidden_layers + 1\n","        weights_init = torch.zeros(n_weights).float()\n","        weights_init.data[:-1] = -3\n","        self.layer_weights = torch.nn.Parameter(weights_init)\n","        \n","        nn.init.normal_(self.linear.weight,\n","                        mean=0.0,\n","                        std=config.initializer_range)\n","        nn.init.zeros_(self.linear.bias)\n","\n","    def forward(self, **inputs):\n","        _, hidden_layers = self.bert(**inputs)\n","        \n","        output = torch.stack(\n","            [self.dropout(layer[:, 0, :]) for layer in hidden_layers], dim=2\n","        )\n","        output = (torch.softmax(self.layer_weights, dim=0) * output).sum(-1)\n","        \n","        output = torch.mean(\n","            torch.stack(\n","                [self.linear(self.high_dropout(output)) for _ in range(5)],\n","                dim=0,\n","            ),\n","            dim=0,\n","        )\n","        return output\n","\n","    def get_grouped_params(self, lr=5e-5, bert_lr=2e-5, bert_lr_decay=0.95):\n","        no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n","        bert_layers_lr = [('embeddings', bert_lr * pow(bert_lr_decay, 11))] + \\\n","                      [(f'layer.{i}.', bert_lr * pow(bert_lr_decay, 11 - i)) for i in range(12)]\n","        bert_layers_name = ['embeddings'] + [f'layer.{i}.' for i in range(12)]\n","        bert_params = []\n","        other_params = []\n","\n","        for g, l in bert_layers_lr:\n","            bert_params.append(\n","                {\n","                    'params': [p for n, p in self.named_parameters() if\n","                               not any(nd in n for nd in no_decay) and any(nd in n for nd in [g])],\n","                    'lr': l, 'weight_decay': 0.01\n","                }\n","            )\n","            bert_params.append(\n","                {\n","                    'params': [p for n, p in self.named_parameters() if\n","                               any(nd in n for nd in no_decay) and any(nd in n for nd in [g])],\n","                    'lr': l, 'weight_decay': 0.0\n","                }\n","            )\n","\n","        other_params = [\n","            {'params': [p for n, p in self.named_parameters() if\n","                        not any(nd in n for nd in no_decay) and not any(nd in n for nd in bert_layers_name)],\n","             'weight_decay': 0.01, 'lr': lr},\n","            {'params': [p for n, p in self.named_parameters() if\n","                        any(nd in n for nd in no_decay) and not any(nd in n for nd in bert_layers_name)],\n","             'weight_decay': 0.0, 'lr': lr},\n","        ]\n","\n","        grouped_params = bert_params + other_params\n","        return grouped_params"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRrL95koNsPi"},"source":["class ClassBalancedLoss(nn.CrossEntropyLoss):\n","    def __init__(self, beta, freq_per_class, device=\"cuda\"):\n","        super().__init__()\n","\n","        assert beta > 0 and beta < 1\n","        weight = [(1-beta)/(1-beta**freq) for freq in freq_per_class]\n","        self.weight = torch.tensor(weight, device=device, requires_grad=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmZ0Faacu_1V"},"source":["class Trainer:\n","    def __init__(self,\n","                 model,\n","                 train_dataloader,\n","                 valid_dataloader,\n","                 criterion,\n","                 optimizer,\n","                 scheduler=None,\n","                 num_epoch=10,\n","                 model_name='./models/best_model'):\n","        \n","        self.model = model\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        if scheduler is not None:\n","            self.scheduler = scheduler\n","        else:\n","            self.scheduler = optim.lr_scheduler.StepLR(self.optimizer,\n","                                                       step_size=1e+10,\n","                                                       gamma=1.0)\n","        self.num_epoch = num_epoch\n","        self.model_name = model_name\n","\n","\n","    def _train_step(self, epoch):\n","        self.model.train()\n","        total_loss = 0\n","        all_labels = np.array([])\n","        all_preds = np.array([])\n","\n","        progress = tqdm(self.train_dataloader, total=len(self.train_dataloader))\n","\n","        for i, batch in enumerate(progress):\n","            progress.set_description(f\"<Train> Epoch{epoch+1}\")\n","\n","            labels = batch.pop('labels')\n","            inputs = batch\n","\n","            self.optimizer.zero_grad()\n","\n","            output = self.model(**inputs)\n","            loss = self.criterion(output, labels)\n","            pred = torch.argmax(output, dim=1)\n","\n","            loss.backward()\n","            self.optimizer.step()\n","            self.scheduler.step()\n","\n","            total_loss += loss.item()\n","            all_labels = np.r_[all_labels, labels.to('cpu').detach().numpy()]\n","            all_preds = np.r_[all_preds, pred.to('cpu').detach().numpy()]\n","            f1 = f1_score(all_labels, all_preds)\n","\n","            progress.set_postfix(loss=total_loss/(i+1), f1=f1)\n","\n","        train_loss = total_loss / len(self.train_dataloader)\n","        train_f1 = f1\n","\n","        return train_loss, train_f1\n","\n","    def _eval_step(self, epoch):\n","        self.model.eval()\n","        total_loss = 0\n","        all_labels = np.array([])\n","        all_preds = np.array([])\n","\n","        with torch.no_grad():\n","            progress = tqdm(self.valid_dataloader,\n","                            total=len(self.valid_dataloader))\n","            \n","            for i, batch in enumerate(progress):\n","                progress.set_description(f\"<Valid> Epoch{epoch+1}\")\n","\n","                labels = batch.pop('labels')\n","                inputs = batch\n","\n","                output = self.model(**inputs)\n","                loss = self.criterion(output, labels)\n","                pred = torch.argmax(output, dim=1)\n","                \n","                total_loss += loss.item()\n","                all_labels = np.r_[all_labels, labels.to('cpu').detach().numpy()]\n","                all_preds = np.r_[all_preds, pred.to('cpu').detach().numpy()]\n","                f1 = f1_score(all_labels, all_preds)\n","\n","                progress.set_postfix(loss=total_loss/(i+1), f1=f1)\n","\n","            valid_loss = total_loss / len(self.valid_dataloader)\n","            valid_f1 = f1\n","\n","        return valid_loss, valid_f1\n","\n","    def train(self, metric='f1'):\n","        if metric == 'f1':\n","            best_metric = 0\n","        elif metric == 'loss':\n","            best_metric = np.inf\n","        else:\n","            raise RuntimeError()\n","\n","        for epoch in range(self.num_epoch):\n","            train_loss, train_f1= self._train_step(epoch)\n","            valid_loss, valid_f1 = self._eval_step(epoch)\n","            print(f'Loss: {valid_loss}  f1: {valid_f1}', end='  ')\n","\n","            if metric == 'f1':\n","                if valid_f1 > best_metric:\n","                    best_metric = valid_f1\n","                    print('model saving!', end='')\n","                    torch.save(self.model.state_dict(), f\"{self.model_name}.pth\")\n","            elif metric == 'loss':\n","                if valid_loss < best_metric:\n","                    best_metric = valid_loss\n","                    print('model saving!', end='')\n","                    torch.save(self.model.state_dict(), f\"{self.model_name}.pth\")\n","            else:\n","                raise RuntimeError()\n","            print('\\n\\n')\n","\n","        return best_metric"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQgel0Esb-lC"},"source":["tokenizer = Tokenizer(MODEL_NAME)\n","# additional_tokens = [' enron ', ' ect ', ' hou ']\n","# tokenizer = Tokenizer(MODEL_NAME, additional_tokens)\n","train_valid_df = pd.read_csv(TRAIN_FILE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETqZUhJpT3AS"},"source":["f1_scores = []\n","skfolds = StratifiedKFold(n_splits=NUM_SPLIT, shuffle=True, random_state=SEED)\n","for fold, (train_index, valid_index) in enumerate(skfolds.split(train_valid_df, train_valid_df['y'])):\n","    print(f\"fold {fold}\", \"=\"*80)\n","\n","    train_df = train_valid_df.iloc[train_index].reset_index(drop=True)\n","    valid_df = train_valid_df.iloc[valid_index].reset_index(drop=True)\n","\n","    train_dataset = make_dataset(train_df, tokenizer, DEVICE)\n","    valid_dataset = make_dataset(valid_df, tokenizer, DEVICE)\n","\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True\n","    )\n","    valid_dataloader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False\n","    )\n","\n","    model = Classifier(MODEL_NAME, num_classes=NUM_CLASS)\n","    model.bert.resize_token_embeddings(len(tokenizer.bert_tokenizer))\n","    model = model.to(DEVICE)\n","    grouped_params = model.get_grouped_params(lr=5e-5, bert_lr=2e-5, bert_lr_decay=0.95)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = AdamW(grouped_params)\n","    scheduler = get_cosine_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0,\n","                                                num_training_steps = 15*len(train_dataloader))\n","    \n","    model_name = MODEL_DIR + MODEL_NAME.replace('/', '_') + '_' + str(fold)\n","    trainer = Trainer(model,\n","                      train_dataloader,\n","                      valid_dataloader,\n","                      criterion,\n","                      optimizer,\n","                      scheduler=scheduler,\n","                      num_epoch=NUM_EPOCH,\n","                      model_name=model_name)\n","    \n","    f1 = trainer.train(metric='f1')\n","    f1_scores.append(f1)\n","    print(f\"<fold={fold}> best score: {f1}\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUOajZycW_dl"},"source":["cv = sum(f1_scores) / len(f1_scores)\n","for i, f1 in enumerate(f1_scores):\n","    print(f\"fold{i}: {f1}\")\n","print(f\"CV:    {cv}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPUBqH-0XdGn"},"source":["lines = \"\"\n","for i, f1 in enumerate(f1_scores):\n","    line = f\"fold={i}: {f1}\\n\"\n","    lines += line\n","lines += f\"CV    : {cv}\"\n","with open(f\"{MODEL_DIR}{MODEL_NAME.replace('/', '_')}_result.txt\", mode='w') as f:\n","    f.write(lines)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPanOAFXXwdO"},"source":["models = []\n","for fold in tqdm(range(NUM_SPLIT)):\n","    model = Classifier(MODEL_NAME)\n","    model.bert.resize_token_embeddings(len(tokenizer.bert_tokenizer))\n","    model.load_state_dict(torch.load(f'{MODEL_DIR}{MODEL_NAME.replace(\"/\", \"_\")}_{fold}.pth'))\n","    model.to(DEVICE)\n","    model.eval()\n","    models.append(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lU2-f8aaX_fE"},"source":["test_df = pd.read_csv(TEST_FILE)\n","test_df['y'] = -1\n","test_dataset = make_dataset(test_df, tokenizer, DEVICE)\n","test_dataloader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZzdwYCjwoxD"},"source":["# 制約付き対数尤度最大化問題を解く\n","def optimize(prob):\n","    logp = np.log(prob + 1e-16)\n","    N = prob.shape[0]\n","    K = prob.shape[1]\n","\n","    m = pulp.LpProblem('Problem', pulp.LpMaximize)  # 最大化問題\n","\n","    # 最適化する変数(= 提出ラベル)\n","    x = pulp.LpVariable.dicts('x', [(i, j) for i in range(N) for j in range(K)], 0, 1, pulp.LpBinary)\n","    \n","    # log likelihood(目的関数)\n","    log_likelihood = pulp.lpSum([x[(i, j)] * logp[i, j] for i in range(N) for j in range(K)])\n","    m += log_likelihood\n","    \n","    # 各データについて，1クラスだけを予測ラベルとする制約\n","    for i in range(N):\n","        m += pulp.lpSum([x[(i, k)] for k in range(K)]) == 1\n","    \n","    # 各クラスについて，推定個数の合計に関する制約\n","    for k in range(K):\n","        m += pulp.lpSum([x[(i, k)] for i in range(N)]) == TEST_FREQS[k]\n","        \n","    m.solve()  # 解く\n","\n","    assert m.status == 1  # assert 最適 <=>（実行可能解が見つからないとエラー）\n","\n","    x_ast = np.array([[int(x[(i, j)].value()) for j in range(K)] for i in range(N)])  # 結果の取得\n","    return x_ast.argmax(axis=1) # 結果をonehotから -> {0, 1}のラベルに変換"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iy2DnMhi8TbN"},"source":["def postprocess(final_output, test_df):\n","    assert final_output.shape[0] == test_df.shape[0]\n","    is_empty = test_df['contents'] == 'Subject: \\r\\n'\n","    final_output[is_empty] = np.array([0.0, 1.0])\n","    return final_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnJQiPoHtble"},"source":["with torch.no_grad():\n","    progress = tqdm(test_dataloader, total=len(test_dataloader))\n","    final_output = np.empty([0,0])\n","\n","    for batch in progress:\n","        progress.set_description(\"<Test>\")\n","\n","        _ = batch.pop('labels')\n","        inputs = batch\n","\n","        outputs = []\n","        for model in models:\n","            output = model(**inputs)\n","            outputs.append(output)\n","\n","        outputs = sum(outputs) / len(outputs)\n","        outputs = torch.softmax(outputs, dim=1).cpu().detach().numpy()\n","        # outputs = np.argmax(outputs, axis=1)\n","\n","        final_output = np.append(final_output, outputs).reshape((-1,2))\n","\n","    final_output = postprocess(final_output, test_df)\n","    final_output = optimize(final_output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUWlaBhftzSp"},"source":["submit = pd.DataFrame(columns=['id', 'y'])\n","submit['id'] = range(1, 24838+1)\n","submit['y'] = final_output\n","try:\n","    submit.to_csv(f\"./output/submission_cv{cv:.10f}_optimized.csv\", index=False)\n","except NameError:\n","    submit.to_csv(\"./output/submission_optimized.csv\", index=False)\n","submit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSVCc6dQ0VTo"},"source":["fig = px.pie(submit, names='y')\n","fig.show()"],"execution_count":null,"outputs":[]}]}